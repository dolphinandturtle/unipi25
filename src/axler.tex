\include{packages}
\include{definitions}
\usepackage[english]{babel}

\begin{document}
\section*{``Linear Algebra Done Right (4th edition)'' by Sheldon Axler}
\begin{multicols}{3}
  1) Linear algebra, the study of linear maps on finite-dimensional vector
  spaces. Complex numbers are needed for some theorems.
  2) Complex numbers purpose, intuition and definition motivated by it.
  3) Properties of complex arithmetic, commutativity proof.
  4) Complex subtraction and division. Field as an alias and scalar power.
  5) Low dimension sets geometry. List definition, notation and difference from
  sets.
  6) Coordinate space definition, geometric intuition limit and coordinate
  addition definition.
  7) Commutativity of coordinate addition, coordinate notation expansion and
  shorthand for additive identity.
  8) Geometric properties of vectors and adding geometrically in $\reals^2$.
  9) Coordinate additive inverse definition and its geometric interpretation in
  $\reals^2$, coordinates scalar multiplication motivation, definition and
  scaling geometrically in $\reals^2$.
  10) Digression on fields, small field and relevance of fields in theorems.
  12) Vector spaces, extracting operation properties from coordinate spaces.
  Addition, scalar multiplication, vector space and vector definitions.
  13) Field choice over scalar multiplication. Fixed field vector spaces.
  coordinate space and sequence space as vector spaces. Set of functions notation
  and operation definition.
  14) Set of functions as vector spaces. Unique additive identity for vector
  spaces.
  15) Unique additive inverse for vector spaces, vector inverse notations,
  vector space absorbent scalar.
  16) Vector space absorbent vector, vector space scalar inversion.
  18) Subspace definition and conditions for it.
  19) Subspace examples, linearity of calculus, subspaces of $\reals^2$ and
  $\reals^3$ . Definition of sum of subspaces.
  20) Sum of subspaces examples.
  21) Sum of subspaces is the smallest containing subspace. Definition of direct
  sum and example.
  22) Example and counter examples for direct sums.
  23) Condition for a direct sum. Direct sum of two subspaces.
  24) Direct sum of multiple subspaces.
  27) Finite-dimensional vector spaces.
  28) List of vectors notation. Definition of linear combination and example.
  29) Definition of span and example. Span is the smallest containing subspace.
  Definition of spanning.
  30) Spanning coordinate space. Definition of finite-dimensional vector space.
  Definition of polynomial space and vector structure and unique coefficients.
  31) Definition of degree of polynomial. Definition of m-polynomial space.
  Definition of infinite-dimensional vector space, like polynomial space.
  Motivating linear indipendence
  32) Definition of linear independence and examples.
  33) Definition of linear dependence and examples. Linear dependence lemma.
  34) Linear dependence lemma edge case and example.
  35) Lenght of linearly independent list greater then lenght of spanning list.
  36) Lenght of vector lists examples. Finite-dimensional subspaces.
  39) Definition of basis and examples. Criterion for basis.
  40) Every spanning list contains a basis.
  41) Basis of finite-dimensional vector space. Every linearly independent list
  extends to a basis.
  42) Every subspace is part of a direct sum equal to the entire space.
  44) Intuition for dimension following coordinate space standard basis.
  Basis lenght does not depend on basis. Definition of dimension and examples.
  45) Dimension of a subspace. Linearly independent list of the right lenght is a
  basis. Subspace of full dimension equals the whole space.
  46) Basis for linearly independent list examples. Spanning list of the right
  lenght is a basis.
  47) Dimension of a sum.
  48) Comparing finite sets with finite-dimensional vector spaces.
  51) Correspondence between linear maps and matrices.
  52) Definition, notation and examples for linear map.
  53) More examples for linear maps.
  54) Linear map lemma.
  55) Definition of addition and scalar multiplication between linear maps.
  The linear map space is a vector space. Definition of product of linear maps.
  56) Algebraic properties of products of linear maps with examples. An linear
  maps takes zero to zero.
  59) Definition and examples of null space. The null space is a subspace.
  60) Definition of injective. Injectivity is equivalent to null space with
  only zero.
  61) Definition and examples of range. The range is a subspace.
  62) Definition and example of surjective. The fundamental theorem of linear
  maps.
  63) Linear map to a lower-dimensional space is not injective and example.
  64) Linear map to a higher-dimensional space is not surjective. Expressing
  systems of linear equations in term of linear maps.
  65) Homogeneous system of linear equations. System of linear equations with
  more equations than variables.
  69) Definition and example of matrix. Definition of matrix of a linear map.
  70) Matrix construction, standard basis and examples.
  71) Definition of matrix addition. Matrix of the sum of linear maps.
  Definition of scalar multiplication of a matrix.
  72) Example of addition and scalar multiplication on matrix. The matrix of a
  scalar times a linear map. Notation for m-by-n matrix space. Dimension of a
  matrix. Motivation for matrix multiplication.
  73) Definition and example of matrix multiplication.
  74) Matrix of product of linear maps. Notation for matrix rows and columns
  and example of usage.
  75) Entry of matrix product equals row times column. Column of matrix product
  equals matrix times column. Motivation for matrix product as linear
  combination.
  76) Linear combination of columns. Hint for linear combination of rows and
  developing tools to proove the column-row factorization. Matrix
  multiplication as linear combinations of columns or rows.
  77) Definition and examples of column rank and row rank. Definition of
  transpose of a matrix.
  78) Example of transpose of a matrix. Column-row factorization. Column rank
  equals row rank.
  79) Definition of rank.
  82) Definition of invertible and inverse linear map. Inverse is unique.
  Notation and example for inverse matrix.
  83) Invertibility equals injectivity and surjectivity.
  84) Counter-examples for invertibility. Injectivity is equivalent to
  surjectivity for linear maps between same finite-dimensional vector spaces.
  85) Example for injectivity-surjectivity equivalence. Inverse matrix
  commutativity over base matrix.
  86) Definition of isomorphism and isomorphic. Isomorphism seen as set
  relabeling. Dimensions shows whether vector spaces are isomorphic.
  87) Isomorphism between vector spaces and coordinate spaces based of their
  dimension. Linear transformations are isomorphic to matrices. Dimension of
  linear transformations.
  88) Definition and example of matrix of a vector. Relabeling elements of a
  vector space once a basis is chosen.
  89) The column of a matrix is a transformed basis. Linear maps act like
  matrix multiplication. Matrices induce linear maps from a vector matrix space
  to another. Chosen a basis every linear map can be seen as a matrix
  multiplication operation.
  90) Dimension of the range of a transformation equals the rank of the matrix
  of that transformation. For linear maps from a vector space to itself we
  usually use the same input and output basis. Definition of identity matrix.
  91) Lax notation for identity operator and matrix. Definition of invertible
  and inverse matrix. Uniqueness of the inverse matrix and inverse of the
  product. Matrix of product of linear maps.
  92) Matrix of identity operator with respect to two bases and example.
  93) Change-of-basis formula. Matrix of inverse equals inverse of matrix.
\end{multicols}

\pagebreak

\mychapter{Propositions}

\begin{multicols}{3}
  \mysection{Vector spaces}

  \mydefinition{Complex numbers}
  Ordered pairs of real numbers with addition and multiplication consistent with
  the algebra of a sum between a number and a scaled \textit{root of the negative unit}.

  \mytheorem{Properties of complex arithmetic}
  Commutativity, associativity, identities, inverses and distributivity.

  \mydefinition{Subtraction/Division}
  Adding/Multiplying by the \textit{inverse} of the operand.

  \mydefinition{List of lenght}
  A \textit{finite} set with an \textit{ordering}.

  \mydefinition{Coordinate space}
  \myparagraph{Definition}
  The set of \textit{scalar lists} of a given lenght.
  \myparagraph{Operations}
  Addition and scalar multiplication are defined \textit{coordinatewise}.
  \myparagraph{Elements}
  The \textit{identity element} is a list of zeros, the \textit{inverse}
  of an element is its coordinatewise \textit{scalar inverse}.

  \mydefinition{Vector operations}
  \myparagraph{Addition}
  A map from a space to \textit{its} endomaps.
  \myparagraph{Scalar multiplication}
  A map from a \textit{field} to the endomaps of a space.

  \mydefinition{Vector space}
  An \textit{abelian group} under addition with a \textit{ring homomorphism} from a \textit{field} into the group's \textit{endomorphism ring} as scalar multiplication.

  \mydefinition{Standard vector spaces}
  A real/complex vector space has scalar multiplication defined on the \textit{real/complex field}.

  \mytheorem{Algebraic properties}
  Identity and inverses are unique. Null scaling and scaling null is null. Inverse identity scaling is inverse.

  \mysection{Subspaces}
  \mydefinition{Subspace}
  A subset of a vector space with its \textit{additive identity}, \textit{operations} and properties.
  \myparagraph{Characterization}
  If and only if it contains the additive identity and is \textit{closed} under the operations.

  \mydefinition{Sum of subspaces}
  The set of all \textit{sums between elements} of distinct subspaces.
  \myparagraph{Structure}
  The smallest \textit{subspace} containing the subspaces \textit{individually}.

  \mydefinition{Direct-sum of subspaces}
  A sum of subspaces whose elements can be written as a \textit{unique sum}.
  \myparagraph{Reciprocal characterization}
  If and only if its \textit{null vector} can be written as a unique sum of null vectors.
  \myparagraph{Set characterization}
  If and only if the \textit{intersection} between the operands is trivial.

  \mysection{Span and linear independence}
  \mydefinition{Linear combination of vectors}
  A \textit{sum over an entrywise products} of scalars and vectors.
  
  \mydefinition{Span}
  The set of all \textit{linear combinations} of a list of vectors.
  \myparagraph{Structure}
  The smallest subspace containing the list of vectors.

  \mydefinition{Polynomial with scalar coefficients}
  A field endo-map that sums over the entrywise products of scalars and increasing \textit{whole powers}.
  \myparagraph{Degree}
  The \textit{greatest power} taken, not scaled by zero.
  
  \mydefinition{Cardinality}
  A vector space \textit{spanned} by some list of vectors is \textit{finite-dimensional}, if not then \textit{infinite-dimensional}.
  
  \mydefinition{Linear dependence}
  A list of vectors whose null linear combination implies all coefficients to be zero is \textit{linearly independent},
  if not then \textit{linearly dependent}.
  \myparagraph{Characterization}
  Some in a linearly dependent list of vectors are in the span of their preceding, \textit{they don't affect the first span}.
  
  \mytheorem{List lenght constraint}
  The lenght of a linearly independent list \textit{cannot be greater} than that of a spanning list.
  
  \mytheorem{Finite-dimensional subspace}
  A subspace of a finite-dimensional vector space is finite-dimensional.
  
  \mysection{Bases}
  \mydefinition{Basis of a vector space}
  A list of vectors that is linearly \textit{independent} and that \textit{spans} their vector space.
  \myparagraph{Characterization}
  If and only if every other vector can be written as a \textit{unique linear combination} of it.
  
  \mytheorem{List resizing to a basis}
  Spanning lists of vectors can be \textit{reduced} to a basis.
  Linearly independent lists of vectors can be \textit{extended} to a basis.

  \mytheorem{Finite-dimensional vector space basis}
  Finite-dimensional vector spaces have a basis.
  
  \mytheorem{Subspace direct completion}
  For every subspace, in a finite-dimensional vector space there exists a \textit{complement} such that their direct sum equals the whole space.
  
  \mysection{Dimension}
  \mytheorem{Basis lenght lemma}
  Any pair of bases of a vector space have the \textit{same lenght}.
  
  \mydefinition{Dimension of a vector space}
  The \textit{lenght} of any basis of the vector space.
  
  \mytheorem{Dimension of a subspace}
  The dimension of a subspace \textit{cannot be greater} than the dimension of the whole space.
  
  \mytheorem{Full dimension subspace}
  A subspace whose dimension is \textit{equal} to that of its whole space, is the whole space itself.
  
  \mytheorem{Basis lenght list}
  A list of vectors whose lenght is equal to the full dimension and is linearly independent or spanning, is also a basis.
  
  \mytheorem{Dimension of a sum}
  The dimension of the sum between a pair of subspaces is the sum of their dimensions minus the dimension of their intersection.

  \mysection{Linear maps}
  \mydefinition{Linear map}
  A \textit{group endomorphism} over addition that \textit{commutes} with scalar multiplication.
  \myparagraph{Characterization}
  Elements of a basis can be mapped everywhere, and the map is unique.
  \myparagraph{Operations}
  Pointwise \textit{addition} and \textit{scalar multiplication}.
  \myparagraph{Structure}
  The space of linear maps with addition and scalar multiplication is a \textit{vector space}.

  \mydefinition{Product of linear maps}
  The product of maps gives their \textit{composition}.
  \myparagraph{Algebraic properties}
  Associativity, identity and distributivity over addition.

  \mytheorem{Mapped null}
  Null vectors map into null vectors.

  \mydefinition{Null space}
  The set of elements mapped into \textit{null vectors} by a given linear map.
  \myparagraph{Structure}
  \textit{Subspace} of the domain.

  \mydefinition{Injective}
  A map whose distinct elements are \textit{distinctly mapped}.
  \myparagraph{Characterization}
  If and only if the null space is trivial.

  \mydefinition{Range}
  The set of elements \textit{mapped} by a given linear map.
  \myparagraph{Structure}
  \textit{Subspace} of the \textit{codomain}.

  \mydefinition{Surjective}
  A map whose \textit{range} equals its \textit{codomain}.

  \mytheorem{Fundamental theorem}
  A linear map's \textit{domain} dimension is equal to the sum between the dimensions of its \textit{null space} and \textit{range}.

  \mytheorem{Map classing}
  In matters of dimension, higher isn't \textit{into} lower and lower isn't \textit{onto} higher.

  \mytheorem{Solution of linear systems}
  \paragraph{\textbf{Homogeneous}}
  A system of linear equations with more variables than equations and null costant terms, has \textit{non-zero solutions}.
  
  \paragraph{\textbf{Eterogeneous}}
  A system of linear equations with more equation than variables has \textit{no solution} for some choice of constant terms.
  
  \mysection{Matrices}
  \mydefinition{Matrix}
  A whole rectangle of numbers with a \textit{row-column} index pair.
  \myparagraph{Operations}
  Elementwise \textit{addition} and \textit{scalar multiplication}.
  
  \mydefinition{Map matrix-form}
  Each column represents a \textit{mapped domain basis} as a linear combination of \textit{codomain basis} with column coefficients.

  \mydefinition{Matrix space}
  A set of matrices of fixed geometry with matrix addition and scalar multiplication.
  \myparagraph{Structure}
  Vector space.
  \myparagraph{Dimension}
  The \textit{row-count} times the \textit{column-count}.
  
  \mydefinition{Matrix multiplication}
  \textit{Entry-wise} sum of the \textit{component-wise} product of row and column.
  
  \mytheorem{Conservation map operations}
  Linear maps are \textit{homomorphic} to matrices over \textit{addition}, \textit{scaling} and \textit{multiplication}.

  \mydefinition{Row/column matrices}
  A matrix with \textit{one row/column}.

  \mytheorem{Matrix multiplication entry}
  Entry of matrix equals row-matrix times column-matrix.

  \mytheorem{Matrix multiplication column}
  Column of matrix product equals matrix times column-matrix.

  \mytheorem{Multiplication by column}
  A linear combination of the matrix's column-matrices with
  column-matrix coefficient.

  \mytheorem{Matrix multiplication as a linear combination}
  Multiplication columns/rows can be written as the linear combinations of
  left/right columns/rows with
  the corresponding right/left column/row coefficients.
  
  \mydefinition{Column/row rank}
  Dimension of the span of the columns/rows of a matrix.

  \mydefinition{Transpose of a matrix}
  A matrix automap that \textit{inverts} rows and columns.

  \mytheorem{Column-row factorization}
  Every matrix with a non trivial column-rank
  can be written as a product between
  a matrix of \textit{rank-width} and a matrix of \textit{rank-height}.

  \mytheorem{Row-column rank}
  The column rank of a matrix \textit{equals} its row rank.

  \mydefinition{Rank}
  The column rank of a matrix.

  \mysection{Isomorphisms}
  \mydefinition{Inverse}
  Given a map its inverse is such that their left/right and right/left give the identity map.
  \myparagraph{Cardinality}
  Given a map the inverse is unique.
  \myparagraph{Characterization}
  If and only if it is injective and surjective.

  \mytheorem{Map class equivalence}
  \textit{Injectivity}, \textit{surjectivity} and \textit{invertibility} in a linear map between two spaces of equal \textit{finite-dimension} are all equivalent.

  \mytheorem{Semi-invertibility}
  If the product of \textit{finite-dimensional} linear maps is the identity, the product \textit{commutes}.

  \mydefinition{Isomorphism}
  An invertible linear map.
  \myparagraph{Existence}
  If two vector spaces have equal and finite dimension.

  \mytheorem{Matrix-form morphism}
  Given a basis of the vector spaces acted upon, the matrix form is an
  \textit{isomorphism} from the space of \textit{linear maps} to the \textit{space of matrices}.

  \mytheorem{Dimension of a linear map}
  A linear map's dimension is the product between the dimension of its \textit{domain} and \textit{codomain}.

  \mydefinition{Vector-matrix}
  A \textit{column-matrix} of coefficients from the linear combination of a basis representing the vector.

  \mytheorem{Matrix column explicitation}
  A column of a matrix-form is the vector-matrix of the corresponding mapped basis.
  
  \mytheorem{Maps as matrix multiplication}
  A linear mapping of a vector is equal to the matrix multiplication between their matrix-form respectively.
  
  \mytheorem{Range-rank}
  Given a \textit{finite-dimensional} linear map,
  the dimension of its \textit{range} is equal to
  the \textit{column rank} of its matrix form.
  
  \mydefinition{Identity matrix}
  Square matrix with left to right diagonal filled with 1's.

  \mydefinition{Inverse matrix}
  Given an \textit{invertible} matrix its inverse is such that their left/right product gives the \textit{idenity matrix}.

  \mytheorem{Matrix of map products}
  The matrix-form of a linear map product is the product of the linear maps' matrix-forms.
  
  \mytheorem{Identity inverse}
  The matrix of an identity map with respect to some bases is
  \textit{its} inverse with inverted bases.
  
  \mytheorem{Change-of-basis formula}
    
  \mytheorem{Matrix of inverse}
  The matrix of the inverse map is the inverse matrix of the map.

  \mysection{Products and Quotients}

  \mydefinition{Product of vector spaces}
  \myparagraph{Definition}
  The set of lists of vectors from the vector spaces respectively.
  \myparagraph{Operations}
  Entrywise \textit{addition} and \textit{scalar multiplication}.
  \myparagraph{Structure} Vector space.
  \myparagraph{Dimension}
  The \textit{sum} of dimensions of the operand vector spaces.

  \mytheorem{Products and direct sums}
  Maps from product of spaces to sums of spaces is direct if and only if it is \textit{injective}.

  \mytheorem{Direct sum dimension}
  Sums between vector spaces is direct if and only if its dimensions \textit{add up}.

  \mydefinition{Translations of subsets}
  The set of all \textit{vector sums} between a vector and the subset elements.

  \mydefinition{Quotient space}
  The \textit{quotient} between a vector space and a subset of his is the set of all
  \textit{translates} of the subsets.
  \myparagraph{Relations}
  Two translates of a subspace are \textit{equal} or \textit{disjoint}.
  \myparagraph{Operations}
  The \textit{sum} of translates is the translates by the sum, the \textit{scaled} translate
  is the translate by the scaled.
  \myparagraph{Structure} Vector space.
  \myparagraph{Dimension}
  The difference between the dimensions of the \textit{dividend} and \textit{divisor} respectively.

  \mydefinition{Quotient map}
  A linear map from a vector space to its quotient that maps vectors
  into the translated subset.

  \mydefinition{Tilde map}
  Linear map on the quotient of space with its kernel that
  sends kernel translates into translation vectors.

  \mytheorem{Kernel and range of tilde map}
  Quotient map into tilde map gives the map, tilde map is injective,
  tilde map's range is equal to that of its map and
  the quotient of the domain with the map kernel is isomorphic
  to its map.

  \mysection{Duality}

  \mydefinition{Linear functional}
  A linear map from a vector space to a field.

  \mydefinition{Dual space}
  The vector space of all linear functionals on a vector space.
  \myparagraph{Dimension}
  The dimension of its vector space.

  \mydefinition{Dual basis}
  Given a basis of a vector space, the basis of its dual
  is a set of functionals that map the starting basis elements
  into 1 and the rest into 0.

  \mytheorem{Coefficient extraction}
  A vector can be expressed as a linear combination of its
  basis with corresponding dual basis elements acting as
  coefficients.

  \mytheorem{Dual basis coherence}
  The dual basis of a vector space is a basis of its dual.

  \mydefinition{Dual map}
  A linear map's dual is a map from the duals of its codomain
  to domain defined by the starting map acted on by an element.

  \mytheorem{Algebraic properties of dual maps}
  Duality \textit{distributes} over addition, scalar multiplication
  and the dual of the product is the \textit{commuted product} of the duals.

  \mydefinition{Annihilator}
  The annihilator of a subspace is the set of all linear functionals that send its elements to zero.
  \myparagraph{Structure} A \textit{subspace} of the subspace's whole space's \textit{dual}.
  \myparagraph{Dimension}
  The difference between the dimensions of the whole space and subspace respectively
  \myparagraph{Bounds}
  The annihilator is \textit{trivial} or the \textit{whole space} if and only if the subspace is the whole space or trivial respectively.

  \mytheorem{Null space of a dual map}
  If the map acts upon finite-dimensional vector spaces
  the kernel of the dual map is the annihilator of the range of the map.
  In matters of dimension, that of the kernel of the dual map is that of
  the kernel of the map plus that of the codomain minus that of the domain.
  
  \mytheorem{Cross dual properties properties}
  If the map acts between finite-dimensional vector spaces then the map is subspace if and only if its dual is injective.

  \mytheorem{The range of the dual}
  If the map acts between finite-dimensional vector spaces then the dimension of the range of the map and of its dual is equal,
  the range of the dual is equal to the annihilator of the kernel of the map.

  \mytheorem{Cross dual properties properties}
  If the map acts between finite-dimensional vector spaces then the map is injective if and only if its dual is surjective.

  \mytheorem{Matrix of the dual}
  If the map acts between finite-dimensional vector spaces, the matrix of the map dual is the transpose of the matrix of the map.
  
  \mytheorem{Row-column rank}
  The column rank of a matrix \textit{equals} its row rank.

  \mysection{Polynomials}
  \mydefinition{Real/immaginary part}
  \mydefinition{Complex conjugate}
  \mydefinition{Absolute value}
  \mytheorem{Properties of complex numbers}
  \mydefinition{Zero of a polynomial}
  \mytheorem{One-degree factoring}
  Each zero of a polynomial corresponds to a degree-one factor.
  \mytheorem{Count of zeros}
  \mytheorem{Division algorithm}
  \mytheorem{Fundamental theorem of algebra}
  \myparagraph{First version}
  \myparagraph{Second version}
  \mytheorem{Complex pairs of zeros}
  \mytheorem{Factorization of a quadratic polynomial}
  \mytheorem{Factorization of a real polynomial}
  
  
  
\end{multicols}

\end{document}
